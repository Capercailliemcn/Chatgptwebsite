<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Brodie's Local AI Chatbot</title>
  <style>
    :root{
      --bg:#0f172a; /* slate-900 */
      --panel:#111827; /* gray-900 */
      --muted:#1f2937; /* gray-800 */
      --ring:#3b82f6; /* blue-500 */
      --text:#e5e7eb; /* gray-200 */
      --text-dim:#94a3b8; /* slate-400 */
      --user:#2563eb; /* blue-600 */
      --bot:#334155; /* slate-700 */
      --accent:#f59e0b; /* amber-500 */
    }
    *{box-sizing:border-box}
    html,body{height:100%;}
    body{margin:0;background:linear-gradient(180deg,var(--bg),#0b1024);color:var(--text);font:16px/1.5 system-ui,Segoe UI,Roboto,Ubuntu,"Helvetica Neue",Arial}
    .wrap{max-width:880px;margin:0 auto;padding:24px;display:flex;flex-direction:column;min-height:100%}
    header{display:flex;align-items:center;gap:12px;margin-bottom:16px}
    header h1{font-size:22px;margin:0}
    header .badge{padding:2px 8px;border-radius:999px;background:var(--muted);color:var(--text-dim);font-size:12px;border:1px solid #263244}
    .card{background:linear-gradient(180deg,var(--panel),#0e162e);border:1px solid #1f2b42;border-radius:16px;box-shadow:0 10px 30px rgba(2,8,23,.6);display:flex;flex-direction:column;min-height:0}
    .toolbar{display:flex;gap:12px;align-items:center;padding:12px;border-bottom:1px solid #1f2b42}
    select,button,input{background:#0e152b;color:var(--text);border:1px solid #21304c;border-radius:10px;padding:10px 12px}
    select:focus,button:focus,input:focus{outline:2px solid var(--ring);outline-offset:2px}
    .toolbar .right{margin-left:auto;display:flex;gap:8px;align-items:center}
    .progress{height:6px;background:#0a1226;border-top:1px solid #1f2b42}
    .bar{height:6px;background:linear-gradient(90deg,var(--accent),#22c55e);width:0%;transition:width .2s ease}
    .chat{padding:18px;display:flex;flex-direction:column;gap:12px;overflow:auto;min-height:360px;max-height:60vh}
    .msg{max-width:80%;padding:12px 14px;border-radius:14px;border:1px solid #24324d;white-space:pre-wrap}
    .user{align-self:flex-end;background:linear-gradient(180deg,#102653,#0d1b35);border-color:#21407a}
    .bot{align-self:flex-start;background:linear-gradient(180deg,var(--bot),#273449)}
    .sys{align-self:center;background:#0a1226;color:var(--text-dim);font-size:13px}
    .input{display:flex;gap:10px;padding:12px;border-top:1px solid #1f2b42}
    .input input{flex:1}
    .hint{font-size:12px;color:var(--text-dim);margin-top:6px}
    .small{font-size:12px;color:var(--text-dim)}
    .hidden{display:none}
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <h1>ðŸ’¬ Brodie's Local AI (runs in your browser)</h1>
      <span class="badge">No API key â€¢ GitHub Pages ready</span>
    </header>

    <div class="card">
      <div class="toolbar">
        <label>
          Model
          <select id="modelSelect">
            <option value="Phi-3-mini-4k-instruct-q4f16_1">Phi-3 Mini (fast, tiny)</option>
            <option value="Llama-3.2-1B-Instruct-q4f16_1">Llama 3.2 1B (small)</option>
            <option value="Llama-3.1-8B-Instruct-q4f16_1">Llama 3.1 8B (slow, best quality)</option>
          </select>
        </label>
        <div class="right">
          <button id="reset">New chat</button>
        </div>
      </div>
      <div class="progress"><div class="bar" id="bar"></div></div>
      <div class="chat" id="chat"></div>
      <div class="input">
        <input id="msg" placeholder="Type your message and hit Enterâ€¦" />
        <button id="send">Send</button>
      </div>
    </div>
    <div class="hint">First load of a model can take a minute while weights download and compile to your GPU. After that, it's snappier. ðŸ˜‰</div>
    <div class="hint">Tip: If it says your browser doesn't support WebGPU, try Chrome/Edge on desktop.
    </div>
  </div>

  <!-- WebLLM: Open-source models running fully in the browser via WebGPU -->
  <script type="module">
    // Import WebLLM from ESM CDN
    import { CreateMLCEngine } from "https://esm.run/@mlc-ai/web-llm";

    const chatEl = document.getElementById('chat');
    const sendBtn = document.getElementById('send');
    const msgInput = document.getElementById('msg');
    const resetBtn = document.getElementById('reset');
    const modelSelect = document.getElementById('modelSelect');
    const bar = document.getElementById('bar');

    // Conversation memory (simple)
    let messages = [{ role: 'system', content: "You are a helpful, friendly assistant for Brodie. Keep answers concise and fun." }];

    // Engine cache per model so switching is faster after first load.
    const engines = new Map();
    let engine = null;

    function addMsg(text, who='bot'){
      const div = document.createElement('div');
      div.className = `msg ${who}`;
      div.textContent = text;
      chatEl.appendChild(div);
      chatEl.scrollTop = chatEl.scrollHeight;
      return div;
    }

    function addSys(text){
      const div = document.createElement('div');
      div.className = 'msg sys';
      div.textContent = text;
      chatEl.appendChild(div);
      chatEl.scrollTop = chatEl.scrollHeight;
      return div;
    }

    async function initEngine(model){
      if(engines.has(model)){
        engine = engines.get(model);
        return engine;
      }
      addSys(`Loading model: ${model}`);
      bar.style.width = '2%';
      engine = await CreateMLCEngine(model, {
        initProgressCallback: (report)=>{
          const p = Math.floor((report.progress || 0) * 100);
          bar.style.width = p + '%';
        },
      });
      engines.set(model, engine);
      setTimeout(()=> bar.style.width = '0%', 600);
      addSys('Model ready. Ask me anything!');
      return engine;
    }

    async function send(){
      const text = msgInput.value.trim();
      if(!text || !engine) return;
      msgInput.value = '';
      addMsg(text,'user');
      messages.push({ role: 'user', content: text });
      const thinking = addMsg('â€¦thinking');

      try{
        // Non-streaming for simplicity
        const out = await engine.chat.completions.create({
          messages,
          model: engine.model,
          temperature: 0.7,
          max_tokens: 256,
        });
        const reply = out.choices?.[0]?.message?.content || "(no reply)";
        thinking.remove();
        addMsg(reply,'bot');
        messages.push({ role: 'assistant', content: reply });
      }catch(err){
        thinking.remove();
        addMsg('Error: '+ err.message, 'bot');
      }
    }

    // Wire up UI
    sendBtn.addEventListener('click', send);
    msgInput.addEventListener('keydown', (e)=>{ if(e.key==='Enter') send(); });
    resetBtn.addEventListener('click', ()=>{ messages = messages.slice(0,1); chatEl.innerHTML=''; addSys('New chat started.'); });

    // Boot: pick default model and load
    (async()=>{
      // Prefer Phi-3 Mini first as it's tiny/fast for most devices
      await initEngine(modelSelect.value);
    })();

    // Switch model handler
    modelSelect.addEventListener('change', async (e)=>{
      await initEngine(e.target.value);
    });
  </script>
</body>
</html>
